{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e44be9-263a-418a-97f3-f4123981ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vision Transformer\n",
    "To classify to disease classes, I am going to use a vision transformer. I chose to use the pretrained model with 86 million parameters and I am also trying smaller versions. This is because it was mentioned in the foundational paper on Vision transformers called An Image is Worth 16x16 Words and the image data set is not large enough to warrant a bigger model, at around 85 thousand samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d678c41-98aa-41de-9a6f-4ebf663b4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\n",
    "    \"google/vit-base-patch16-224-in21k\",    #86M params, baseline, traiend on 14M images w/ 21k classes\n",
    "    \n",
    "    \"WinKawaks/vit-small-patch16-224\",       #22M params, faster training\n",
    "    \"WinKawaks/vit-tiny-patch16-224\" # smallest model.\n",
    "    \"google/vit-base-patch16-224\"           #86M params, fine tuned on ImageNet-1k classes. DECIDED AGAINST IT\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f8302-a993-4dfd-88e2-ba0d106dfaee",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e057e9e5-fcbe-4d2d-b97e-48149c3cdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, Trainer\n",
    "from torchvision.datasets import ImageFolder # for dataset loading\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm #progress bar for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ece015-a91f-4a1a-b3b8-7fcb0f829080",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9bb97c44-11d1-4a85-8d22-7099565ca0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPath = \"data/New Plant Diseases Dataset(Augmented)/train\"\n",
    "validPath = \"data/New Plant Diseases Dataset(Augmented)/valid\"\n",
    "#basic transformation\n",
    "processed = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  #ViT expects 224x224. It is in the model's name!\n",
    "    transforms.ToTensor(),          #converts image to tensor: <0-1> range of values\n",
    "      transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])#pixel value of 0 becomes -1, 0.5->0, 1->1\n",
    "\n",
    "])\n",
    "trainDataset = ImageFolder(root=trainPath, transform=processed)\n",
    "validDataset = ImageFolder(root=validPath, transform=processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8c503a8-81fa-4c82-83b5-62494c35a861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 70295\n",
      "Validation samples: 17572\n",
      "Classes: 38\n",
      "First 5 class names: ['Apple___Apple_scab', 'Apple___Black_rot', 'Apple___Cedar_apple_rust', 'Apple___healthy', 'Blueberry___healthy']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training samples: {len(trainDataset)}\")\n",
    "print(f\"Validation samples: {len(validDataset)}\")\n",
    "print(f\"Classes: {len(trainDataset.classes)}\")\n",
    "print(f\"First 5 class names: {trainDataset.classes[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f3f3d3-0544-46ac-9aee-b92b7b7eb99a",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac902f2-2fc8-474d-937a-28c6dad8ed01",
   "metadata": {},
   "source": [
    "The model was pretrained with specific mean/std values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8063c0f-1969-43ba-86d5-12ce025e61e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected image size: {'height': 224, 'width': 224}\n",
      "stamdard deviation: [0.5, 0.5, 0.5]\n",
      "Mean: [0.5, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "processor = ViTImageProcessor.from_pretrained('WinKawaks/vit-tiny-patch16-224')\n",
    "print(\"Expected image size: \" + str(processor.size))\n",
    "print(\"stamdard deviation: \"+str(processor.image_std))\n",
    "print(\"Mean: \" + str(processor.image_mean)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99839a9-5c65-4e8c-897d-1e0eca39da70",
   "metadata": {},
   "source": [
    "Now that I found out what mean and sd of the transformer are, I added it back into the previous transform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200a4a4-61ab-4432-bcbb-ecb16589f56a",
   "metadata": {},
   "source": [
    "# WinKawaks/vit-tiny-patch16-224\n",
    "source: https://huggingface.co/WinKawaks/vit-tiny-patch16-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6abd76a9-c6b5-42db-8f1b-84497b86788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([38]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([38, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pretrained model. I am going to try a couple more of these to find the ebst result\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'WinKawaks/vit-tiny-patch16-224',\n",
    "    num_labels=38,  # this corresponds to the number of disease classes, discovered with EDA\n",
    "    ignore_mismatched_sizes=True #the model expects 1000 classes just like in ImgNet, \n",
    "    #so the final layer has to bereplaced\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89eb6e-103b-4249-aee5-050c85456103",
   "metadata": {},
   "source": [
    "The new classification head is randomly initialized, so I will need to fine tune it in order for it to work, as stated in the paper:\n",
    "\n",
    "*Typically,  we  pre-train  ViT  on  large  datasets,  and  fine-tune  to  (smaller)  downstream  tasks.   For this, we remove the pre-trained prediction head and attach a zero-initialized D×K feedforward layer, where K is the number of downstream classes.  It is often beneficial to fine-tune at higher resolution than pre-training*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b51f52-d3cd-4320-9121-4d1b683a9f88",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5cd6137b-23e6-4ab0-9667-dbb0604e0cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batches amount: 2197\n",
      "Validation batches amount: 550\n"
     ]
    }
   ],
   "source": [
    "batchSize = 32 # my memory is limited by my inadequate laptop\n",
    "trainLoader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "validLoader = DataLoader(validDataset, batch_size=batchSize, shuffle=False)\n",
    "print('Training batches amount: ' + str(len(trainLoader)))#2197\n",
    "print('Validation batches amount: ' + str(len(validLoader)))# 550, checks out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf545b1-8fdf-4229-989d-30daa3e6251c",
   "metadata": {},
   "source": [
    "The paper says:\n",
    "*We fine-tune all ViT models using SGD with a momentum of 0.9.*\n",
    "\n",
    "\n",
    "For learning rates, they tried: {0.001, 0.003, 0.01, 0.03} depending on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "24d0ca48-a3f3-4ad9-9733-4297b3dbd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.003, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss() #typical for classification\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#i dont have an nVidia gpu, so cpu it is\n",
    "model = model.to(device)               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41de71e8-e1b2-4788-b2db-7af239bb0d71",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "ba53fd67-c5a7-4a60-bc63-b3eaf86fdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(model, trainLoader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    cumuLoss = float(0)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch in tqdm(trainLoader, desc=\"Training model...\"):\n",
    "        images, labels = batch\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        #nulling gradients:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #forward pass\n",
    "        #returns an object with logits inside it\n",
    "        outputs = model(images)\n",
    "        logits = outputs.logits\n",
    "        #backward pass\n",
    "        loss = criterion(logits, labels)\n",
    "        # loss calculation for this step       \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #metrics update\n",
    "        cumuLoss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        correct += (predicted == labels).sum().item() # if prediction matches label, it counts\n",
    "        total += labels.size(0)\n",
    "    epochLoss = runningLoss /len(trainLoader)\n",
    "    epochAccuracy = correct/total\n",
    "    return epochLoss, epochAccuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5984d8f-35b3-43ff-93e2-ee5ad346a5aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training (1 epoch test)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training model...:   7%|████▎                                                     | 161/2197 [08:29<1:52:50,  3.33s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Starting training (1 epoch test)\")\n",
    "loss, acc = trainEpoch(model, trainLoader, optimizer, criterion, device)\n",
    "print(f\"Loss: {loss:.4f}, Accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d644154-b794-4dd2-9151-1f7e71bbe0dd",
   "metadata": {},
   "source": [
    "## google/vit-base-patch16-224\n",
    "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\n",
    "source:  https://huggingface.co/google/vit-base-patch16-224-in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784fc6d-d66a-46a6-8309-be4b64bf7371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
