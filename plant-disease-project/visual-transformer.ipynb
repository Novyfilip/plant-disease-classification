{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76e44be9-263a-418a-97f3-f4123981ac93",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Vision Transformer\n",
    "To classify to disease classes, I am going to use a vision transformer. I chose to use the pretrained model with 86 million parameters and I am also trying smaller versions. This is because it was mentioned in the foundational paper on Vision transformers called An Image is Worth 16x16 Words and the image data set is not large enough to warrant a bigger model, at around 85 thousand samples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e057e9e5-fcbe-4d2d-b97e-48149c3cdec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import ViTForImageClassification, ViTImageProcessor, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d678c41-98aa-41de-9a6f-4ebf663b4d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = [\n",
    "    \"google/vit-base-patch16-224-in21k\",    #86M params, baseline, traiend on 14M images w/ 21k classes\n",
    "    \n",
    "    \"WinKawaks/vit-small-patch16-224\",       #22M params, faster training\n",
    "    \"WinKawaks/vit-tiny-patch16-224\" # smallest model.\n",
    "    \"google/vit-base-patch16-224\"           #86M params, fine tuned on ImageNet-1k classes. DECIDED AGAINST IT\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d200a4a4-61ab-4432-bcbb-ecb16589f56a",
   "metadata": {},
   "source": [
    "# WinKawaks/vit-tiny-patch16-224\n",
    "source: https://huggingface.co/WinKawaks/vit-tiny-patch16-224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6abd76a9-c6b5-42db-8f1b-84497b86788d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at WinKawaks/vit-tiny-patch16-224 and are newly initialized because the shapes did not match:\n",
      "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([38]) in the model instantiated\n",
      "- classifier.weight: found shape torch.Size([1000, 192]) in the checkpoint and torch.Size([38, 192]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# pretrained model. I am going to try a couple more of these to find the ebst result\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    'WinKawaks/vit-tiny-patch16-224',\n",
    "    num_labels=38,  # this corresponds to the number of disease classes, discovered with EDA\n",
    "    ignore_mismatched_sizes=True #the model expects 1000 classes just like in ImgNet, \n",
    "    #so the final layer has to bereplaced\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89eb6e-103b-4249-aee5-050c85456103",
   "metadata": {},
   "source": [
    "The new classification head is randomly initialized, so I will need to fine tune it in order for it to work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437c8cfb-10b6-4835-9c8b-61ac2e19df48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d644154-b794-4dd2-9151-1f7e71bbe0dd",
   "metadata": {},
   "source": [
    "## google/vit-base-patch16-224\n",
    "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.\n",
    "source:  https://huggingface.co/google/vit-base-patch16-224-in21k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784fc6d-d66a-46a6-8309-be4b64bf7371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
